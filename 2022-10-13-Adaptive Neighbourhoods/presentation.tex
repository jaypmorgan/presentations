% Created 2022-10-12 Wed 19:54
% Intended LaTeX compiler: pdflatex
\documentclass[smaller]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{tikz}
\usepackage{tabularx,booktabs,multirow,adjustbox}
\usefonttheme{serif}
\usetheme{default}
\author{Jay Morgan, University of Toulon}
\date{13th October 2022}
\title{Adaptive Neighbourhoods for the Discovery of Adversarial Examples}
\hypersetup{
 pdfauthor={Jay Morgan, University of Toulon},
 pdftitle={Adaptive Neighbourhoods for the Discovery of Adversarial Examples},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 28.1 (Org mode 9.5.5)}, 
 pdflang={English}}
\begin{document}

\maketitle

\section{Introduction}
\label{sec:org95dae40}

\begin{frame}[label={sec:orgda7c189}]{A thank you to my collaborators}
\begin{columns}
\begin{column}{0.3\columnwidth}
\begin{center}
\includegraphics[width=0.7\textwidth]{images/Adeline-Paiement.jpg}
\end{center}

\begin{center}
Adeline Paiement
University of Toulon
\end{center}
\end{column}

\begin{column}{0.3\columnwidth}
\begin{center}
\includegraphics[width=0.7\textwidth]{images/Arno-Pauly.jpg}
\end{center}

\begin{center}
Arno Pauly

Swansea University
\end{center}
\end{column}

\begin{column}{0.3\columnwidth}
\begin{center}
\includegraphics[width=0.7\textwidth]{images/Monika-Seisenberger.jpg}
\end{center}

\begin{center}
Monika Seisenberger
Swansea University
\end{center}
\end{column}
\end{columns}
\end{frame}

\begin{frame}[label={sec:orgf6978f4}]{Deep Learning models}
The abilities of Deep Learning models have only continued to improve, and the range
of tasks they can perform is growing: from simple digit recognition, to simultaneous
detection of multiple objects in a scene.

\begin{columns}
\begin{column}{0.3\columnwidth}
\begin{center}
\includegraphics[width=.9\linewidth]{images/mnist.png}
\end{center}
\end{column}

\begin{column}{0.7\columnwidth}
\begin{center}
\includegraphics[width=.9\linewidth]{images/Object-detection-in-a-dense-scene.jpg}
\end{center}
(Potdar, Kedar and Pai, Chinmay and Akolkar, Sukrut, 2018)
\end{column}
\end{columns}
\end{frame}

\begin{frame}[label={sec:org3b504d9}]{Adversarial Examples}
Adversarial examples are created by changing pixel values in the input image,
resulting in an output image that looks almost identical but the Deep Learning model
predicts and entirely different class for this output image.

\begin{center}
\includegraphics[width=.9\linewidth]{images/fgsm_panda_image.png}
\end{center}
(Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian, 2014)
\end{frame}

\begin{frame}[label={sec:org4fda18b}]{Motivating Principles}
For safety critical systems, miss-classifications are more catastrophic.

\begin{center}
\includegraphics[width=.9\linewidth]{images/signs.png}
\end{center}
(Huang, Xiaowei and Kwiatkowska, Marta and Wang, Sen and Wu, Min, 2017)
\end{frame}

\section{Adaptive Neighbourhoods}
\label{sec:orge927552}

\begin{frame}[label={sec:orga966cae}]{Outline for this talk}
\begin{enumerate}
\item Look at existing solutions
\item Our complimentary method
\item Some results on two tasks:
\begin{itemize}
\item Iris Dataset
\item Solar Burst Detection
\end{itemize}
\item Some conclusions
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:org657c1a1}]{Fast Gradient Sign Method (FGSM)}
\begin{center}
\includegraphics[width=.9\linewidth]{images/fgsm_panda_image.png}
\end{center}
(Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian, 2014)
\end{frame}

\begin{frame}[label={sec:org4cec575}]{Projected Gradient Descent (PGD)}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.5\textwidth]{images/projected-gradient-descent.png}
\caption{\url{https://towardsdatascience.com/know-your-enemy-7f7c5038bdf3}}
\end{figure}

(Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian, 2017)
\end{frame}

\begin{frame}[label={sec:org6b6f282}]{What do we learn from these methods?}
\begin{center}
\includegraphics[width=0.5\textwidth]{images/perturbation.png}
\end{center}
\end{frame}

\begin{frame}[label={sec:orgcca2f10}]{Amount of change is important}
\begin{center}
\includegraphics[width=.9\linewidth]{images/eos.png}
\end{center}
\end{frame}

\begin{frame}[label={sec:org8d9e799}]{How to decide maximum perturbation for non-image representations}
\begin{center}
\includegraphics[width=0.7\textwidth]{images/iris.png}
\end{center}
\end{frame}

\section{Results}
\label{sec:org6f03364}

\begin{frame}[label={sec:org85f8e78},plain,c]{}
\begin{center}
\vspace{1em}\Huge Our method -- Adaptive Neighbourhoods
\end{center}
\end{frame}

\begin{frame}[label={sec:orga4f2c86}]{Perturbations shouldn't pass class boundaries}
\begin{figure}
    \centering
    \begin{tikzpicture}[scale=1.3]
        \draw [very thick,dotted] (-2,1.2) .. controls (0.8, 0.8) and (0.8,0) .. (2,-1.5);
        \filldraw [gray] (-0.2,-0.2) circle (3pt);
        \draw [->] (-0.5,-0.5) -- (0.6,0.6);
        \draw [->] (-0.5,-0.5) -- (-1,-1);
        \draw [thick] (-0.2,-0.2) circle (35pt);
        
        \node at (-0.05,-0.55) {$x_i$};
        \node at (-1.7, -0.2) {$\varepsilon$};
        \node[align=center] at (2.8, -0.5) {True class \\ boundary};
    \end{tikzpicture}
    \label{fig:complexity}
\end{figure}

\begin{center}
\vspace{2em}
Example where a data point \(x_i\) lies close to the class decision boundary. In these
situations, too large \(\varepsilon\) values, may push the synthetically generated
point over true class boundaries.
\end{center}
\end{frame}

\begin{frame}[label={sec:org4e03420}]{Estimated boundaries can be deceiving}
\begin{columns}
\begin{column}{0.5\columnwidth}
\begin{figure}
    \centering
    \begin{tikzpicture}[scale=1.3]
	% nodes
	\draw (-1,0) circle (3pt);
	\filldraw [gray] (1,0) circle (3pt);

	%lines
	\draw (0, 1) -- (0,-1);
	\draw [dotted] (-0.9,1) -- (-0.9,-1);
	\draw [dotted] ( 0.9,1) -- ( 0.9,-1);
    \end{tikzpicture}
    \label{fig:density_a}
\end{figure}

\begin{center}
\vspace{2em}Sparse regions of the manifold may appear simple due to the lack of information.
\end{center}
\end{column}

\begin{column}{0.5\columnwidth}
\begin{figure}
    \centering
    \begin{tikzpicture}[scale=1.3]
	% nodes
	\draw (-1,0) circle (3pt);
	\draw (0.5,0.9) circle (3pt);
	\draw (0.5,-0.9) circle (3pt);
	\filldraw [gray] (1,0) circle (3pt);

	%lines
	\draw (0.9, 1) .. controls (0.5,0) .. (0.9,-1);
	\draw [dotted] (1.1, 1) .. controls (0.7,0) .. (1.1,-1);
	\draw [dotted] (0.7, 1) .. controls (0.3,0) .. (0.7,-1);
    \end{tikzpicture}
    \label{fig:density_b}
\end{figure}


\begin{center}
\vspace{2em}More data points enable more precise estimation of the class boundary.
\end{center}
\end{column}
\end{columns}
\end{frame}

\begin{frame}[label={sec:org39e1d0d}]{Estimating Sparsity/Density}
\begin{equation}
    \varphi(x; \overline{x}) =  \frac{1}{\sqrt{1 + (\varepsilon r)^2}},\; \text{where}\; r = \parallel \overline{x} - x \parallel
    \label{eq:rbf}
\end{equation}

\vspace{2em}

We achieve a good measure of the density through the sum of the RBFs centred on all
data points \(X^c\) of class \(c\) (Eq.\textasciitilde{}\ref{eq:density}).

\vspace{2em}

\begin{equation}
    \rho_c(x) = \sum_{x_j \in X^c} \varphi(x; x_j)
    \label{eq:density}
\end{equation}
\end{frame}

\begin{frame}[label={sec:org1030207}]{Iterative expansion to create `adapted neighbourhoods'}
\begin{figure}
    \centering
    \begin{tikzpicture}[scale=.8]
        \draw (0.4,0) node {$x_1$};
        \draw[dashed] (0,0) circle (1.0cm);
        \draw[dashed] (0,0) circle (1.45cm);
        \draw[thick,dotted] (0,0) circle (1.75cm);
        
        \draw[->]        (0.1,0) -- (-1.0,0) node[below,midway] {$\varepsilon_1$};
        \draw[->] (-1.0,0) -- (-1.45,0) node[below,midway] {};
        \draw[->] (-1.5,0) -- (-1.75,0) node[below,midway] {};
        \draw[thick, ->] (0.1,0) -- (-0.5,1.75) node[anchor=south] {$\varepsilon$};
        
        \draw (2.57,1) node {$x_2$};
        \draw[thick] (2.57,1) circle (1.0cm);
        
        \draw (2,-0.4) node {$x_3$};
        \draw[thick,dotted] (2,-0.4) circle (0.5cm);
    \end{tikzpicture}
\label{fig:e_expansion}
\end{figure}

\begin{center}
\vspace{2em}Iterative \(\varepsilon\)-expansion process in a binary class scenario. The
two classes are distinguished by the dotted and solid circles.\vspace{1em}
\end{center}

\begin{equation*}
    \Delta\varepsilon_i^n=e^{-\rho_{c(i)}(x_i) \cdot n}
    \label{eq:step}
\end{equation*}
\end{frame}

\section{Results}
\label{sec:org95f06ef}

\begin{frame}[label={sec:orgf29c018},plain,c]{}
\begin{center}
\vspace{1em}\Huge Results
\end{center}
\end{frame}

\begin{frame}[label={sec:orgbb6578c}]{Aim of Experimentation}
We'd like to answer the following:
\begin{enumerate}
\item Does using adaptive neighbourhood provide any benefit? Why use it at all?
\item Can existing methods work for non-image based datasets, or do we need to design
new methods entirely?
\end{enumerate}
\end{frame}




\begin{frame}[label={sec:orge9d0460}]{Classification of Iris flowers -- problem statement}
\begin{columns}
\begin{column}{0.3\columnwidth}
\begin{center}
\includegraphics[width=1.0\textwidth]{images/Petal-sepal.jpg}
\end{center}
\end{column}

\begin{column}{0.7\columnwidth}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/iris-eps.png}
    \label{fig:iris}
\end{figure}

(Jay Morgan and Adeline Paiement and Arno Pauly and Monika Seisenberger, 2021)
\end{column}
\end{columns}
\end{frame}

\begin{frame}[label={sec:org75a0b42}]{Attack and defence results for the Iris dataset classification task}
\begin{table}
\centering
\begin{adjustbox}{center}
\resizebox{\textwidth}{!}{\begin{tabular}[t]{cccccc}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{4}{c}{Attack} \\
\cmidrule(l{3pt}r{3pt}){3-6}
Defence & None & FGSM & PGD & FGSM+AN & PGD+AN\\
\midrule
None & 0.9745 (0.0413) & \textcolor{red}{0.9278 (0.0618)} & 0.8572 (0.1036) & \textcolor{red}{0.7764 (0.0813)} & 0.8461 (0.0968)\\
FGSM & 0.9811 (0.0396) & 0.9408 (0.0757) & 0.8468 (0.1080) & 0.7873 (0.0785) & 0.8448 (0.0698)\\
PGD & 0.9867 (0.0400) & 0.9462 (0.0740) & 0.8680 (0.0740) & 0.8508 (0.0746) & 0.8759 (0.0823)\\
\midrule
Random+AN & 0.9936 (0.0193) & 0.9272 (0.0620) & 0.8274 (0.0918) & 0.7935 (0.0822) & 0.8454 (0.0864)\\
FGSM+AN & 0.9936 (0.0193) & \textcolor{red}{0.9406 (0.0745)} & 0.8420 (0.0987) & \textcolor{red}{0.8140 (0.1085)} & 0.8588 (0.1157)\\
PGD+AN & 0.9936 (0.0193) & 0.9472 (0.0642) & 0.9472 (0.0642) & 0.8679 (0.0899) & 0.8753 (0.0864)\\
\bottomrule
\end{tabular}}
\end{adjustbox}
\end{table}

\begin{center}
What we learn here then is that our adaptive neighbourhoods is able to strengthen the
form of adversarial attack and defence.
\end{center}
\end{frame}

\begin{frame}[label={sec:orge92ca2e}]{Adversarial examples in a Solar Burst Detection task -- problem statement}
\begin{figure}[t]
\centering
\resizebox{0.55\textwidth}{!}{\input{./images/adversarial_example}}
\label{fig:adv_example}
\end{figure}
(Jay Morgan, 2022)
\end{frame}

\begin{frame}[label={sec:org23d4987}]{Attack and defence results for the solar bursts task}
\begin{table}
\centering
\begin{adjustbox}{center}
\resizebox{\textwidth}{!}{\begin{tabular}[t]{rccccccc}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{6}{c}{Attack} \\
\cmidrule(l{3pt}r{3pt}){3-8}
Defence & None & FGSM & FGSM+AN & PGD & PGD+AN & DAG & DAG+AN\\
\midrule
None & 0.568 & \textcolor{red}{0.539} & \textcolor{red}{0.486} & 0.198 & 0.105 & 0.399 & 0.251\\
FGSM & 0.463 & \textcolor{red}{0.458} & 0.178 & 0.013 & 0.012 & 0.055 & 0.028\\
FGSM+AN & 0.480 & \textcolor{red}{0.465} & 0.462 & 0.007 & 0.007 & 0.043 & 0.023\\
PGD & 0.421 & 0.425 & 0.379 & 0.391 & 0.359 & 0.378 & 0.259\\
PGD+AN & 0.364 & 0.359 & 0.330 & 0.339 & 0.324 & 0.330 & 0.212\\
\bottomrule
\end{tabular}}
\end{adjustbox}
\end{table}

\begin{center}
Like our previous task, we see that, through the combination with adaptive
neighbourhoods, the attack is more successful. And likewise the defence is more
powerful.
\end{center}
\end{frame}

\section{Conclusion}
\label{sec:org915a0e0}

\begin{frame}[label={sec:org3a26648}]{Summary of Results}
We'd like to answer the following:
\begin{enumerate}
\item Does using adaptive neighbourhood provide any benefit? Why use it at all? -
\alert{Adaptive neighbourhoods is an effective method that compliments existing adversarial generation methods such as FGSM \& PGD.}
\item Can existing methods work for non-image based datasets, or do we need to design
new methods entirely? - \alert{Through the use of adaptive neighbourhoods, one can
meaningfully define searchable regions for datasets other than image-based data
where adversarial examples can be visually inspected.}
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:org093242c}]{Source code}
\begin{center}
\includegraphics[width=0.8\textwidth]{images/github-repo.png}
\end{center}

\begin{center}
\url{https://github.com/jaypmorgan/adaptive-neighbourhoods}
\url{https://gibtlab.com/jaymorgan/adaptive-neighbourhoods}
\url{https://git.sr.ht/~jaymorgan/adaptive-neighbourhoods}
\end{center}
\end{frame}

\begin{frame}[label={sec:orgbcbd8ce}]{Link to the Slides}
\begin{center}
\includegraphics[width=1.0\textwidth]{images/presentations.png}
\end{center}

\begin{center}
\url{https://github.com/jaypmorgan/presentations}
\end{center}
\end{frame}

\begin{frame}[label={sec:org24fee9d},plain,c]{}
\begin{center}
\vspace{1em}\Huge Thank you!
\end{center}
\end{frame}

\begin{frame}[label={sec:org1daeff4}]{References}
\noindent
Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian (2014). \emph{Explaining and harnessing adversarial examples}, arXiv preprint arXiv:1412.6572.

\noindent
Huang, Xiaowei and Kwiatkowska, Marta and Wang, Sen and Wu, Min (2017). \emph{Safety verification of deep neural networks}.

\noindent
Jay Morgan (2022). \emph{Strategies to use Prior Knowledge to Improve the Performance of Deep Learning}.

\noindent
Jay Morgan and Adeline Paiement and Arno Pauly and Monika Seisenberger (2021). \emph{Adaptive Neighbourhoods for the Discovery of Adversarial Examples}, CoRR.

\noindent
Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian (2017). \emph{Towards deep learning models resistant to adversarial attacks}, arXiv preprint arXiv:1706.06083.

\noindent
Potdar, Kedar and Pai, Chinmay and Akolkar, Sukrut (2018). \emph{A Convolutional Neural Network based Live Object Recognition System as Blind Aid}.
\end{frame}
\end{document}
