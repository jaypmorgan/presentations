% Created 2022-10-12 Wed 14:06
% Intended LaTeX compiler: pdflatex
\documentclass[smaller]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{tikz}
\usepackage{tabularx,booktabs,multirow,adjustbox}
\usefonttheme{serif}
\usetheme{default}
\author{Jay Morgan, University of Toulon}
\date{13th October 2022}
\title{Adaptive Neighbourhoods for the Discovery of Adversarial Examples}
\hypersetup{
 pdfauthor={Jay Morgan, University of Toulon},
 pdftitle={Adaptive Neighbourhoods for the Discovery of Adversarial Examples},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 28.1 (Org mode 9.5.5)}, 
 pdflang={English}}
\begin{document}

\maketitle

\section{Introduction}
\label{sec:org31a335a}

\begin{frame}[label={sec:org99dcfe8}]{A thank you to my collaborators}
\begin{columns}
\begin{column}{0.3\columnwidth}
\begin{center}
\includegraphics[width=0.7\textwidth]{images/Adeline-Paiement.jpg}
\end{center}

\begin{center}
Adeline Paiement
University of Toulon
\end{center}
\end{column}

\begin{column}{0.3\columnwidth}
\begin{center}
\includegraphics[width=0.7\textwidth]{images/Arno-Pauly.jpg}
\end{center}

\begin{center}
Arno Pauly
Swansea University
\end{center}
\end{column}

\begin{column}{0.3\columnwidth}
\begin{center}
\includegraphics[width=0.7\textwidth]{images/Monika-Seisenberger.jpg}
\end{center}

\begin{center}
Monika Seisenberger
Swansea University
\end{center}
\end{column}
\end{columns}
\end{frame}

\begin{frame}[label={sec:org3bbab8c}]{Deep Neural Networks}
\begin{center}
\includegraphics[width=.9\linewidth]{images/Object-detection-in-a-dense-scene.jpg}
\end{center}
(Potdar, Kedar and Pai, Chinmay and Akolkar, Sukrut, 2018)
\end{frame}

\begin{frame}[label={sec:org35dff4c}]{Adversarial Examples}
\begin{center}
\includegraphics[width=.9\linewidth]{images/fgsm_panda_image.png}
\end{center}
(Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian, 2014)
\end{frame}

\begin{frame}[label={sec:orga8f8786}]{Motivating Principles}
\begin{center}
\includegraphics[width=.9\linewidth]{images/signs.png}
\end{center}
(Huang, Xiaowei and Kwiatkowska, Marta and Wang, Sen and Wu, Min, 2017)
\end{frame}

\section{Adaptive Neighbourhoods}
\label{sec:org3b04df0}

\begin{frame}[label={sec:orge7a7eec}]{Outline for this talk}
\begin{enumerate}
\item Look at existing solutions
\item Our complimentary method
\item Some results on two tasks:
\begin{itemize}
\item Iris Dataset
\item Solar Burst Detection
\end{itemize}
\item Some conclusions
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:orgc756f58}]{Fast Gradient Sign Method (FGSM)}
\begin{center}
\includegraphics[width=.9\linewidth]{images/fgsm_panda_image.png}
\end{center}
(Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian, 2014)
\end{frame}

\begin{frame}[label={sec:org78bd5a9}]{Projected Gradient Descent (PGD)}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.5\textwidth]{images/projected-gradient-descent.png}
\caption{\url{https://towardsdatascience.com/know-your-enemy-7f7c5038bdf3}}
\end{figure}

(Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian, 2017)
\end{frame}

\begin{frame}[label={sec:org4f5db5c}]{What do we learn from these methods?}
\begin{center}
\includegraphics[width=0.5\textwidth]{images/perturbation.png}
\end{center}
\end{frame}

\begin{frame}[label={sec:orgf39bfe0}]{Amount of change is important}
\begin{center}
\includegraphics[width=.9\linewidth]{images/eos.png}
\end{center}
\end{frame}

\begin{frame}[label={sec:org7c9eb37}]{Non-image representations}
\begin{center}
\includegraphics[width=0.7\textwidth]{images/iris.png}
\end{center}
\end{frame}

\begin{frame}[label={sec:orgc0e1f6c}]{Perturbations shouldn't pass class boundaries}
\begin{figure}
    \centering
    \begin{tikzpicture}[scale=1.3]
        \draw [very thick,dotted] (-2,1.2) .. controls (0.8, 0.8) and (0.8,0) .. (2,-1.5);
        \filldraw [gray] (-0.2,-0.2) circle (3pt);
        \draw [->] (-0.5,-0.5) -- (0.6,0.6);
        \draw [->] (-0.5,-0.5) -- (-1,-1);
        \draw [thick] (-0.2,-0.2) circle (35pt);
        
        \node at (-0.05,-0.55) {$x_i$};
        \node at (-1.7, -0.2) {$\varepsilon$};
        \node[align=center] at (2.8, -0.5) {True class \\ boundary};
    \end{tikzpicture}
    \label{fig:complexity}
\end{figure}

\begin{center}
\vspace{2em}
Example where a data point \(x_i\) lies close to the class decision boundary. In these
situations, too large \(\varepsilon\) values, may push the synthetically generated
point over true class boundaries.
\end{center}
\end{frame}

\begin{frame}[label={sec:org92627eb}]{Estimated boundaries can be deceiving}
\begin{columns}
\begin{column}{0.5\columnwidth}
\begin{figure}
    \centering
    \begin{tikzpicture}[scale=1.3]
	% nodes
	\draw (-1,0) circle (3pt);
	\filldraw [gray] (1,0) circle (3pt);

	%lines
	\draw (0, 1) -- (0,-1);
	\draw [dotted] (-0.9,1) -- (-0.9,-1);
	\draw [dotted] ( 0.9,1) -- ( 0.9,-1);
    \end{tikzpicture}
    \label{fig:density_a}
\end{figure}

\begin{center}
\vspace{2em}Sparse regions of the manifold may appear simple due to the lack of information.
\end{center}
\end{column}

\begin{column}{0.5\columnwidth}
\begin{figure}
    \centering
    \begin{tikzpicture}[scale=1.3]
	% nodes
	\draw (-1,0) circle (3pt);
	\draw (0.5,0.9) circle (3pt);
	\draw (0.5,-0.9) circle (3pt);
	\filldraw [gray] (1,0) circle (3pt);

	%lines
	\draw (0.9, 1) .. controls (0.5,0) .. (0.9,-1);
	\draw [dotted] (1.1, 1) .. controls (0.7,0) .. (1.1,-1);
	\draw [dotted] (0.7, 1) .. controls (0.3,0) .. (0.7,-1);
    \end{tikzpicture}
    \label{fig:density_b}
\end{figure}


\begin{center}
\vspace{2em}More data points enable more precise estimation of the class boundary.
\end{center}
\end{column}
\end{columns}
\end{frame}

\begin{frame}[label={sec:org61681e9}]{Estimating Sparsity/Density}
\begin{equation}
    \varphi(x; \overline{x}) =  \frac{1}{\sqrt{1 + (\varepsilon r)^2}},\; \text{where}\; r = \parallel \overline{x} - x \parallel
    \label{eq:rbf}
\end{equation}

\vspace{2em}

We achieve a good measure of the density through the sum of the RBFs centred on all
data points \(X^c\) of class \(c\) (Eq.\textasciitilde{}\ref{eq:density}).

\vspace{2em}

\begin{equation}
    \rho_c(x) = \sum_{x_j \in X^c} \varphi(x; x_j)
    \label{eq:density}
\end{equation}
\end{frame}

\begin{frame}[label={sec:orge6f0228}]{Expansion}
\begin{figure}
    \centering
    \begin{tikzpicture}[scale=.8]
        \draw (0.4,0) node {$x_1$};
        \draw[dashed] (0,0) circle (1.0cm);
        \draw[dashed] (0,0) circle (1.45cm);
        \draw[thick,dotted] (0,0) circle (1.75cm);
        
        \draw[->]        (0.1,0) -- (-1.0,0) node[below,midway] {$\varepsilon_1$};
        \draw[->] (-1.0,0) -- (-1.45,0) node[below,midway] {};
        \draw[->] (-1.5,0) -- (-1.75,0) node[below,midway] {};
        \draw[thick, ->] (0.1,0) -- (-0.5,1.75) node[anchor=south] {$\varepsilon$};
        
        \draw (2.57,1) node {$x_2$};
        \draw[thick] (2.57,1) circle (1.0cm);
        
        \draw (2,-0.4) node {$x_3$};
        \draw[thick,dotted] (2,-0.4) circle (0.5cm);
    \end{tikzpicture}
\label{fig:e_expansion}
\end{figure}

\begin{center}
\vspace{2em}Iterative \(\varepsilon\)-expansion process in a binary class scenario. The
two classes are distinguished by the dotted and solid circles.\vspace{1em}
\end{center}

\begin{equation*}
    \Delta\varepsilon_i^n=e^{-\rho_{c(i)}(x_i) \cdot n}
    \label{eq:step}
\end{equation*}
\end{frame}

\section{Results}
\label{sec:org341e1f0}

\begin{frame}[label={sec:orgd704695},plain,c]{}
\begin{center}
\vspace{1em}\Huge Results
\end{center}
\end{frame}


\begin{frame}[label={sec:orgd3afe91}]{Iris Dataset}
\begin{columns}
\begin{column}{0.3\columnwidth}
\begin{center}
\includegraphics[width=1.0\textwidth]{images/Petal-sepal.jpg}
\end{center}
\end{column}

\begin{column}{0.7\columnwidth}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/iris-eps.png}
    \label{fig:iris}
\end{figure}

(Jay Morgan and Adeline Paiement and Arno Pauly and Monika Seisenberger, 2021)
\end{column}
\end{columns}
\end{frame}

\begin{frame}[label={sec:org824848f}]{Results}
\begin{table}

\caption{\label{tab:irir_results}$F_1$ score of DNN for the Iris dataset using various adversarial defence methods. Scores are in the format: mean (standard deviation) over 10 k-folds. Bold font face indicates the best form of attack for each type of defence method.}
\centering
\begin{adjustbox}{center}
\resizebox{\textwidth}{!}{\begin{tabular}[t]{cccccc}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{4}{c}{Attack} \\
\cmidrule(l{3pt}r{3pt}){3-6}
Defence & None & FGSM & PGD & FGSM+AN & PGD+AN\\
\midrule
None & 0.9745 (0.0413) & 0.9278 (0.0618) & 0.8572 (0.1036) & \textbf{0.7764 (0.0813)} & 0.8461 (0.0968)\\
FGSM & 0.9811 (0.0396) & 0.9408 (0.0757) & 0.8468 (0.1080) & \textbf{0.7873 (0.0785)} & 0.8448 (0.0698)\\
PGD & 0.9867 (0.0400) & 0.9462 (0.0740) & 0.8680 (0.0740) & \textbf{0.8508 (0.0746)} & 0.8759 (0.0823)\\
\midrule
Random+AN & 0.9936 (0.0193) & 0.9272 (0.0620) & 0.8274 (0.0918) & \textbf{0.7935 (0.0822)} & 0.8454 (0.0864)\\
FGSM+AN & 0.9936 (0.0193) & 0.9406 (0.0745) & 0.8420 (0.0987) & \textbf{0.8140 (0.1085)} & 0.8588 (0.1157)\\
PGD+AN & 0.9936 (0.0193) & 0.9472 (0.0642) & 0.9472 (0.0642) & \textbf{0.8679 (0.0899)} & 0.8753 (0.0864)\\
\bottomrule
\end{tabular}}
\end{adjustbox}
\end{table}
\end{frame}


\begin{frame}[label={sec:orgcc6d709}]{Adversarial Training for Solar Burst Detection}
\begin{figure}[t]
\centering
\resizebox{0.55\textwidth}{!}{\input{./images/adversarial_example}}
\label{fig:adv_example}
\end{figure}
(Jay Morgan, 2022)
\end{frame}

\begin{frame}[label={sec:org352b01d}]{Results}
\begin{table}
\caption{\label{tab:adv_fscore}$F_1$ score performance on the WAVES dataset using Faster R-CNN. Numbers highlighted in a bold font face indicate the best achieving adversarial attack for each form of defence.}
\centering
\begin{adjustbox}{center}
\resizebox{\textwidth}{!}{\begin{tabular}[t]{rccccccc}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{6}{c}{Attack} \\
\cmidrule(l{3pt}r{3pt}){3-8}
Defence & None & FGSM & FGSM+AN & PGD & PGD+AN & DAG & DAG+AN\\
\midrule
None & 0.568 & 0.539 & 0.486 & 0.198 & \textbf{0.105} & 0.399 & 0.251\\
FGSM & 0.463 & 0.458 & 0.178 & 0.013 & \textbf{0.012} & 0.055 & 0.028\\
FGSM+AN & 0.480 & 0.465 & 0.462 & \textbf{0.007} & \textbf{0.007} & 0.043 & 0.023\\
PGD & 0.421 & 0.425 & 0.379 & 0.391 & 0.359 & 0.378 & \textbf{0.259}\\
PGD+AN & 0.364 & 0.359 & 0.330 & 0.339 & 0.324 & 0.330 & \textbf{0.212}\\
\bottomrule
\end{tabular}}
\end{adjustbox}
\end{table}
\end{frame}

\section{Conclusion}
\label{sec:orgd6b8901}

\begin{frame}[label={sec:org37c7592}]{Summary of Results}
\begin{itemize}
\item Adaptive neighbourhoods is an effective method that compliments existing
adversarial generation methods such as FGSM \& PGD.
\item Through the use of adaptive neighbourhoods, one can meaningfully define searchable
regions for datasets other than image-based data where adversarial examples can be
visually inspected.
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org0d7f017}]{Source code}
\begin{center}
\includegraphics[width=0.8\textwidth]{images/github-repo.png}
\end{center}

\begin{center}
\url{https://github.com/jaypmorgan/adaptive-neighbourhoods}
\url{https://gibtlab.com/jaymorgan/adaptive-neighbourhoods}
\url{https://git.sr.ht/~jaymorgan/adaptive-neighbourhoods}
\end{center}
\end{frame}

\begin{frame}[label={sec:org37f0ba7}]{Link to the Slides}
\begin{center}
\includegraphics[width=1.0\textwidth]{images/presentations.png}
\end{center}

\begin{center}
\url{https://github.com/jaypmorgan/presentations}
\end{center}
\end{frame}

\begin{frame}[label={sec:org1146510},plain,c]{}
\begin{center}
\vspace{1em}\Huge Thank you!
\end{center}
\end{frame}

\begin{frame}[label={sec:org9c6c0d0}]{References}
\noindent
Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian (2014). \emph{Explaining and harnessing adversarial examples}, arXiv preprint arXiv:1412.6572.

\noindent
Huang, Xiaowei and Kwiatkowska, Marta and Wang, Sen and Wu, Min (2017). \emph{Safety verification of deep neural networks}.

\noindent
Jay Morgan (2022). \emph{Strategies to use Prior Knowledge to Improve the Performance of Deep Learning}.

\noindent
Jay Morgan and Adeline Paiement and Arno Pauly and Monika Seisenberger (2021). \emph{Adaptive Neighbourhoods for the Discovery of Adversarial Examples}, CoRR.

\noindent
Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian (2017). \emph{Towards deep learning models resistant to adversarial attacks}, arXiv preprint arXiv:1706.06083.

\noindent
Potdar, Kedar and Pai, Chinmay and Akolkar, Sukrut (2018). \emph{A Convolutional Neural Network based Live Object Recognition System as Blind Aid}.
\end{frame}
\end{document}
